{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install multiprocess --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import figure\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "import seaborn as sns\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data as csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_table('/Users/gracer/Google Drive/ABCD/important_txt/4Kmeans.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dudes = data[data['sex'] == 0]\n",
    "lady_dudes = data[data['sex'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a List of variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names=list(dudes.columns.values)\n",
    "cols = dudes.columns\n",
    "dudes.columns = ['sub','pds_ht2_y',  'pds_skin2_y',  'pds_bdyhair_y','labels_true',\n",
    "            'pds_f4_2_y',  'pds_f5_y',  'pds_m4_y', 'pds_m5_y', 'interview_age', 'gender',\n",
    "             'anthroheightcalc',  'anthroweightcalc',  'anthro_waist_cm',\n",
    "             'hormone_scr_dhea_mean',  'hormone_scr_hse_mean',  'hormone_scr_ert_mean',  'sex']\n",
    "# print(dudes)\n",
    "# cols[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine variables of interest into a single matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_true=dudes['labels_true'].values\n",
    "subs=dudes['sub'].values\n",
    "f2=dudes['pds_ht2_y'].values\n",
    "f3=dudes['pds_skin2_y'].values\n",
    "f4=dudes['pds_bdyhair_y'].values\n",
    "f5=dudes['pds_m4_y'].values\n",
    "f6=dudes['pds_m5_y'].values\n",
    "f7=dudes['interview_age'].values\n",
    "f8=dudes['anthroheightcalc'].values \n",
    "f9=dudes['anthroweightcalc'].values\n",
    "f10=dudes['anthro_waist_cm'].values\n",
    "f11=dudes['hormone_scr_dhea_mean'].values\n",
    "f12=dudes['hormone_scr_hse_mean'].values\n",
    "X=np.matrix(zip(f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of the variable names included in this analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=['pds_ht2_y',\n",
    "'pds_skin2_y',\n",
    "'pds_bdyhair_y',\n",
    "'pds_m4_y',\n",
    "'pds_m5_y',\n",
    "'interview_age',\n",
    "'anthroheightcalc', \n",
    "'anthroweightcalc',\n",
    "'anthro_waist_cm',\n",
    "'hormone_scr_dhea_mean',\n",
    "'hormone_scr_hse_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 917), (2, 354), (3, 14), (4, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADhxJREFUeJzt3V2MXVd5xvH/Qxw+JFoM8UAj22WQ\n8AW0KpBaqatIFUqoFBIUR2oipWrBQakstVQFUYkaLoqoepHcEERbgVKCaiiFRAE1bhJUpfkQ6gWB\nSQiB4NK4KCWjRHggiQFRqAxvL84yTCfHnj0fx2fO0v8njc7ea685+11e9jNr9jn7OFWFJKlfz5t2\nAZKkyTLoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ3bNu0CAHbs2FHz8/PTLkOS\nZsqDDz743aqaW63flgj6+fl5FhYWpl2GJM2UJP89pJ+XbiSpcwa9JHXOoJekzhn0ktQ5g16SOmfQ\nS1LnDHpJ6pxBL0mdM+glqXNb4s5Yrc38oTundu7Hr798aueWtD6u6CWpcwa9JHXOoJekzhn0ktQ5\ng16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnRsc\n9EnOSfKVJHe0/VcleSDJY0luSfL81v6Ctn+sHZ+fTOmSpCHWsqJ/J3B02f4NwI1VtQd4BriutV8H\nPFNVrwZubP0kSVMyKOiT7AIuBz7W9gNcDNzWuhwGrmzb+9s+7fglrb8kaQqGrug/BLwH+FnbPw94\ntqpOtv1FYGfb3gk8AdCOn2j9/58kB5MsJFlYWlpaZ/mSpNWsGvRJ3gIcr6oHlzeP6VoDjv2ioeqm\nqtpbVXvn5uYGFStJWrttA/pcBFyR5DLghcAvM1rhb0+yra3adwFPtv6LwG5gMck24CXA05teuSRp\nkFVX9FX13qraVVXzwDXAvVX1B8B9wFWt2wHg9rZ9pO3Tjt9bVc9Z0UuSzo6NvI/+L4B3JznG6Br8\nza39ZuC81v5u4NDGSpQkbcSQSzc/V1X3A/e37W8BF47p82Pg6k2oTZK0CbwzVpI6Z9BLUucMeknq\nnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z\n9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEv\nSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXOrBn2SFyb5UpKvJnk0\nyQda+6uSPJDksSS3JHl+a39B2z/Wjs9PdgiSpDMZsqL/CXBxVb0OeD1waZJ9wA3AjVW1B3gGuK71\nvw54pqpeDdzY+kmSpmTVoK+RH7bdc9tXARcDt7X2w8CVbXt/26cdvyRJNq1iSdKaDLpGn+ScJA8D\nx4G7gf8Cnq2qk63LIrCzbe8EngBox08A521m0ZKk4QYFfVX9tKpeD+wCLgReM65bexy3eq+VDUkO\nJllIsrC0tDS0XknSGq3pXTdV9SxwP7AP2J5kWzu0C3iybS8CuwHa8ZcAT495rpuqam9V7Z2bm1tf\n9ZKkVQ15181cku1t+0XAm4CjwH3AVa3bAeD2tn2k7dOO31tVz1nRS5LOjm2rd+F84HCScxj9YLi1\nqu5I8g3gM0n+GvgKcHPrfzPwySTHGK3kr5lA3ZKkgVYN+qp6BHjDmPZvMbpev7L9x8DVm1KdJGnD\nvDNWkjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z\n6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXPbpl3ALJs/dOe0S5CkVbmi\nl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ2b+bdX+hZHSTozV/SS1DmDXpI6Z9BLUucMeknqnEEv\nSZ0z6CWpcwa9JHXOoJekzhn0ktS5VYM+ye4k9yU5muTRJO9s7S9LcneSx9rjS1t7knw4ybEkjyS5\nYNKDkCSd3pAV/Ungz6vqNcA+4B1JXgscAu6pqj3APW0f4M3AnvZ1EPjIplctSRps1aCvqqeq6qG2\n/QPgKLAT2A8cbt0OA1e27f3AJ2rki8D2JOdveuWSpEHWdI0+yTzwBuAB4BVV9RSMfhgAL2/ddgJP\nLPu2xdYmSZqCwUGf5MXAZ4F3VdX3z9R1TFuNeb6DSRaSLCwtLQ0tQ5K0RoOCPsm5jEL+U1X1udb8\nnVOXZNrj8da+COxe9u27gCdXPmdV3VRVe6tq79zc3HrrlyStYsi7bgLcDBytqg8uO3QEONC2DwC3\nL2t/W3v3zT7gxKlLPJKks2/IfzxyEfBW4GtJHm5t7wOuB25Nch3wbeDqduwu4DLgGPAj4O2bWrEk\naU1WDfqq+nfGX3cHuGRM/wLescG6JEmbxDtjJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknq\nnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6tyQ/3hE+rn5Q3dO5byPX3/5VM4r\n9cAVvSR1zhW9ZoK/SUjr54pekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6\nZ9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6tyqQZ/k40mOJ/n6\nsraXJbk7yWPt8aWtPUk+nORYkkeSXDDJ4iVJqxuyov8H4NIVbYeAe6pqD3BP2wd4M7CnfR0EPrI5\nZUqS1mvVoK+qLwBPr2jeDxxu24eBK5e1f6JGvghsT3L+ZhUrSVq79V6jf0VVPQXQHl/e2ncCTyzr\nt9jaJElTstkvxmZMW43tmBxMspBkYWlpaZPLkCSdst6g/86pSzLt8XhrXwR2L+u3C3hy3BNU1U1V\ntbeq9s7Nza2zDEnSatYb9EeAA237AHD7sva3tXff7ANOnLrEI0majm2rdUjyaeCNwI4ki8D7geuB\nW5NcB3wbuLp1vwu4DDgG/Ah4+wRqliStwapBX1W/f5pDl4zpW8A7NlqUJGnzeGesJHXOoJekzhn0\nktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9J\nnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5\ng16SOrdt2gVIW9n8oTundu7Hr798audWX1zRS1LnXNFLW9S0fpvwN4n+uKKXpM4Z9JLUOYNekjpn\n0EtS5wx6SeqcQS9JnTPoJalzEwn6JJcm+WaSY0kOTeIckqRhNv2GqSTnAH8H/C6wCHw5yZGq+sZm\nn0tSX7xJbDImsaK/EDhWVd+qqv8FPgPsn8B5JEkDTOIjEHYCTyzbXwR+awLnkaRN0fuH100i6DOm\nrZ7TKTkIHGy7P0zyzXWebwfw3XV+71bjWLaeXsYBA8eSG85CJRvXzbzkhg2N5ZVDOk0i6BeB3cv2\ndwFPruxUVTcBN230ZEkWqmrvRp9nK3AsW08v4wDHslWdjbFM4hr9l4E9SV6V5PnANcCRCZxHkjTA\npq/oq+pkkj8F/hU4B/h4VT262eeRJA0zkc+jr6q7gLsm8dxjbPjyzxbiWLaeXsYBjmWrmvhYUvWc\n10klSR3xIxAkqXMzEfRJPp7keJKvn+Z4kny4feTCI0kuONs1DjVgLG9MciLJw+3rL892jUMl2Z3k\nviRHkzya5J1j+mz5uRk4jpmYlyQvTPKlJF9tY/nAmD4vSHJLm5MHksyf/UpXN3As1yZZWjYvfzSN\nWodIck6SryS5Y8yxyc5JVW35L+B3gAuAr5/m+GXA5xm9h38f8MC0a97AWN4I3DHtOgeO5Xzggrb9\nS8B/Aq+dtbkZOI6ZmJf25/zitn0u8ACwb0WfPwE+2ravAW6Zdt0bGMu1wN9Ou9aB43k38E/j/h5N\nek5mYkVfVV8Anj5Dl/3AJ2rki8D2JOefnerWZsBYZkZVPVVVD7XtHwBHGd0ZvdyWn5uB45gJ7c/5\nh2333Pa18oW4/cDhtn0bcEmScTc6TtXAscyEJLuAy4GPnabLROdkJoJ+gHEfuzCT/1Cb326/rn4+\nya9Nu5gh2q+ab2C06lpupubmDOOAGZmXdongYeA4cHdVnXZOquokcAI47+xWOcyAsQD8XrsseFuS\n3WOObwUfAt4D/Ow0xyc6J70E/aCPXZgRDwGvrKrXAX8D/POU61lVkhcDnwXeVVXfX3l4zLdsyblZ\nZRwzMy9V9dOqej2ju9IvTPLrK7rMzJwMGMu/APNV9RvAv/GLVfGWkeQtwPGqevBM3ca0bdqc9BL0\ngz52YRZU1fdP/bpao/sRzk2yY8plnVaScxmF46eq6nNjuszE3Kw2jlmbF4Cqeha4H7h0xaGfz0mS\nbcBL2OKXE083lqr6XlX9pO3+PfCbZ7m0IS4CrkjyOKNP8704yT+u6DPROekl6I8Ab2vv8NgHnKiq\np6Zd1Hok+ZVT1+aSXMhojr433arGa3XeDBytqg+eptuWn5sh45iVeUkyl2R7234R8CbgP1Z0OwIc\naNtXAfdWexVwKxkylhWv91zB6PWVLaWq3ltVu6pqntELrfdW1R+u6DbROZnInbGbLcmnGb3rYUeS\nReD9jF6Yoao+yugu3MuAY8CPgLdPp9LVDRjLVcAfJzkJ/A9wzVb8R9hcBLwV+Fq7jgrwPuBXYabm\nZsg4ZmVezgcOZ/QfAD0PuLWq7kjyV8BCVR1h9EPtk0mOMVo1XjO9cs9oyFj+LMkVwElGY7l2atWu\n0dmcE++MlaTO9XLpRpJ0Gga9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0md+z843aRau64B\n+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111cbefd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(labels_true)\n",
    "y = np.bincount(labels_true.astype(int))\n",
    "ii = np.nonzero(y)[0]\n",
    "zip(ii,y[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue is a low number of people in groups 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible solution, randomly sample equal numbers\n",
    "Using the rule of thumb 2^m I need 8 people per cluster \n",
    "Possible combinations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393849377343759797528386895216640000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "x=math.factorial(70)\n",
    "y=math.factorial(70-20)\n",
    "fact=x/y\n",
    "print(fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target variable (or the one you are comparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_var=pd.DataFrame(dudes['labels_true'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "This will not allow missing data, so have to impute nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer()\n",
    "transformed_values = imputer.fit_transform(X)\n",
    "# count the number of NaN values in each column\n",
    "print(np.isnan(transformed_values).sum()) \n",
    "transformed_values_scale = scale(transformed_values)\n",
    "#the target variable is the last variable\n",
    "trans = np.hstack((transformed_values_scale,target_var.round(decimals=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1286, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to randomly sample the data and perform the kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bench_k_means(estimator, name, data):\n",
    "        t0 = time() #time\n",
    "        estimator.fit(data) #estimating the fit \n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=sample_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meaner(**kwargs):\n",
    "    for x in kwargs.items():\n",
    "        print(x)\n",
    "#     z=sum(*arg)/len(*arg)\n",
    "#     return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_reducer(DATA):\n",
    "    n_samples, n_features = DATA.shape\n",
    "    labels = np.round(targets)\n",
    "    n_digits = len(np.unique(targets))\n",
    "    sample_size=n_samples\n",
    "    \n",
    "    PCA_results = PCA(n_components=2)\n",
    "    reduced_data = PCA_results.fit_transform(DATA)\n",
    "    \n",
    "    # Dump components relations with features:\n",
    "    print pd.DataFrame(PCA_results.components_,index = ['PC-1','PC-2'])\n",
    "    plt.semilogy(PCA_results.explained_variance_ratio_, '--o')\n",
    "    return (PCA_results.components_, PCA_results.explained_variance_ratio_, \n",
    "            PCA_results.explained_variance_, PCA_results.mean_) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_list(a_list):\n",
    "        half = len(a_list)/2\n",
    "        return a_list[:half], a_list[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_selct(DATA,i):\n",
    "    dictr = {}\n",
    "    #defining the levels, based on the final column add the values to the dictionary\n",
    "    levels = ['lev1','lev2','lev3']\n",
    "    i=1\n",
    "    for lev in levels:\n",
    "        if i < len(levels)+1:\n",
    "            dictr[lev] = DATA[np.where(DATA[:,-1] == i)]\n",
    "            i=i+1\n",
    "    \n",
    "    rand_dict={}\n",
    "    target_dict={}\n",
    "    ind_dict={}\n",
    "    for key, value in dictr.iteritems():\n",
    "        #shuffle the data's index\n",
    "        ind = np.random.permutation(value.shape[0])#random index\n",
    "        #get the first 20 subjects indexes\n",
    "        training_idx = ind[:50]#get 20 subjects indexes\n",
    "        #get the first 20 subjects\n",
    "        training = value[training_idx,:]#select 20 subjects from the value in the dictionary\n",
    "        #saving the true labels\n",
    "        labels_true = training[:,-1] #get the labels from the value in the dictiornary last column\n",
    "        target_dict[key] = labels_true #add targets to dictionary\n",
    "        rand_dict[key] = training #match the randomized data to the ind_dict by key \n",
    "        ind_dict[key] = training_idx #add the indexes to the dictionary\n",
    "    \n",
    "    '''\n",
    "    combine all the dictionaries we have created thus far. \n",
    "    data, index, and targets\n",
    "    '''\n",
    "    \n",
    "    #combine the randomized data by the actual level\n",
    "    data=np.vstack((rand_dict['lev1'],rand_dict['lev2'],rand_dict['lev3']))\n",
    "    \n",
    "    ###################################################\n",
    "    data=np.delete(data,11,1)# remove the label column\n",
    "    ###################################################\n",
    "    \n",
    "    \n",
    "    #combine the true labels into targets\n",
    "    targets=np.hstack((target_dict['lev1'],target_dict['lev2'],target_dict['lev3']))\n",
    "    #combine the indexes into one\n",
    "    indexes=np.hstack((ind_dict['lev1'],ind_dict['lev2'],ind_dict['lev3']))\n",
    "    \n",
    "    '''\n",
    "    defining the parameters for the k means and the PCA\n",
    "    '''\n",
    "    \n",
    "    n_samples, n_features = data.shape\n",
    "    labels = np.round(targets)\n",
    "    n_digits = len(np.unique(targets))\n",
    "    sample_size=n_samples\n",
    "\n",
    "    '''\n",
    "    defining the parameters for the k means\n",
    "    '''    \n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=300)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################\n",
    "    #            rebuild to combine data      #\n",
    "    ###########################################\n",
    "    comb_data = np.column_stack((data, kmeans.fit_predict(data)))#makes first column the index value\n",
    "    full_data = np.column_stack((indexes, comb_data))#makes first column the index value\n",
    "    full_data = np.column_stack((full_data,labels))\n",
    "    comb_dictr = {}\n",
    "    '''\n",
    "    seperating the data based on the fit predict value (found right above)\n",
    "    '''\n",
    "    i=0\n",
    "    for lev in levels:\n",
    "        if i < len(levels)+1:\n",
    "            comb_dictr[lev] = comb_data[np.where(comb_data[:,-1] == i)]\n",
    "            i=i+1\n",
    "    '''\n",
    "    defining the parameters for the PCA and running the PCA \n",
    "    '''\n",
    "    ###########################################\n",
    "    #            PCA                          #\n",
    "    ###########################################\n",
    "    PCA_dict={}\n",
    "    PCA_data={}\n",
    "    PCA_final = {}\n",
    "\n",
    "    for key, value in comb_dictr.iteritems():\n",
    "        PCA_dict[key] = PCA(n_components=2)\n",
    "        #make sure the last column which is the predicted label isn't included\n",
    "        PCA_data[key] = PCA_dict[key].fit_transform(value[:,0:11])\n",
    "        #make sure the last column which is the predicted label isn't included\n",
    "        ID = np.identity(value[:,0:11].shape[1])  # identity matrix\n",
    "        coef = PCA_dict[key].transform(ID)\n",
    "        PCA_final[key] = coef\n",
    "        \n",
    "        \n",
    "        plt.semilogy(PCA_dict[key].explained_variance_ratio_, '--o', label = '%s'%key)\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "\n",
    "    reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "    kmeans.fit(reduced_data)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "              'Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "#     plt.show()\n",
    "    \n",
    "    ####THINGS TO SAVE#############################################################################        \n",
    "    kmeans.fit(data)\n",
    "    y_kmeans = kmeans.predict(data)\n",
    "    clusters = kmeans.fit_predict(data)\n",
    "    cluster_space = kmeans.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    centroids = kmeans.cluster_centers_   \n",
    "    \n",
    "    Components = np.vstack((PCA_dict['lev1'].components_,\n",
    "                            PCA_dict['lev2'].components_,PCA_dict['lev3'].components_))\n",
    "    \n",
    "    Var_ratio = PCA_final\n",
    "    \n",
    "    lev1_VarRatio = np.vstack((PCA_final['lev1']))\n",
    "    lev2_VarRatio = np.vstack((PCA_final['lev2']))\n",
    "    lev3_VarRatio = np.vstack((PCA_final['lev3']))\n",
    "    \n",
    "    ind_dict = np.hstack((ind_dict['lev1'],\n",
    "                            ind_dict['lev2'],ind_dict['lev3']))\n",
    "    \n",
    "    ########################################################################################################\n",
    "    return (clusters, cluster_space, centroids, \n",
    "            targets, ind_dict,\n",
    "           Components, Var_ratio, \n",
    "           full_data,\n",
    "           lev1_VarRatio,\n",
    "           lev2_VarRatio,\n",
    "           lev3_VarRatio,\n",
    "           kmeans,\n",
    "           DATA)\n",
    "\n",
    "# 0clusters\n",
    "# 1cluster_space, \n",
    "# 2centroids, \n",
    "# 3targets, \n",
    "# 4ind_dict,\n",
    "# 5Components,\n",
    "# 6Var_ratio, \n",
    "# 7full_data,\n",
    "# 8lev1_VarRatio,\n",
    "# 9lev2_VarRatio,\n",
    "# 10lev3_VarRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(i):\n",
    "    names = ['pds_ht2_y',\n",
    "'pds_skin2_y',\n",
    "'pds_bdyhair_y',\n",
    "'pds_m4_y',\n",
    "'pds_m5_y',\n",
    "'interview_age',\n",
    "'anthroheightcalc', \n",
    "'anthroweightcalc',\n",
    "'anthro_waist_cm',\n",
    "'hormone_scr_dhea_mean',\n",
    "'hormone_scr_hse_mean',\n",
    "'labels_pred',\n",
    "'labels_true']\n",
    "    \n",
    "    Clusters = []\n",
    "    Cluster_space = []\n",
    "    Centroids = []\n",
    "    \n",
    "    Comp = []\n",
    "    Var_ratio = []\n",
    "    \n",
    "    Targets = []\n",
    "    Indexes = []\n",
    "    \n",
    "    Full_data = []\n",
    "    \n",
    "    Lev1_VarRatio = []\n",
    "    Lev2_VarRatio = []\n",
    "    Lev3_VarRatio = []\n",
    "    \n",
    "    kmeans = []\n",
    "    for x in range(0,i):\n",
    "        print(x)\n",
    "        Q = random_selct(trans,i)\n",
    "        Clusters.append(Q[0])\n",
    "        Cluster_space.append(Q[1])\n",
    "        Centroids.append(Q[2])\n",
    "        \n",
    "        Targets.append(Q[3])\n",
    "        Indexes.append(Q[4])\n",
    "        \n",
    "        Comp.append(Q[5])\n",
    "        Var_ratio.append(Q[6])\n",
    "        \n",
    "        Full_data.append(Q[7])\n",
    "        \n",
    "        Lev1_VarRatio.append(Q[8].transpose())\n",
    "        Lev2_VarRatio.append(Q[9].transpose())\n",
    "        Lev3_VarRatio.append(Q[10].transpose())\n",
    "        \n",
    "        kmeans.append(Q[11])\n",
    "    All_lev1Ratio = np.vstack(Lev1_VarRatio)\n",
    "    All_lev2Ratio = np.vstack(Lev2_VarRatio)\n",
    "    All_lev3Ratio = np.vstack(Lev3_VarRatio)\n",
    "    \n",
    "    All_clust = np.hstack(Clusters)\n",
    "    All_clust_space = np.vstack(Cluster_space)\n",
    "    \n",
    "    All_labels = np.hstack(Targets)\n",
    "    All_index = np.hstack(Indexes)\n",
    "    \n",
    "    All_comp = np.vstack(Comp)\n",
    "    All_var_ratio = Var_ratio\n",
    "    All_data = np.vstack(Full_data)\n",
    "    print(All_data)\n",
    "    print(All_data.shape)\n",
    "    All_data=pd.DataFrame(data=All_data[1:,1:],index=All_data[1:,0], columns= names)\n",
    "    return (All_clust ,All_clust_space,  \n",
    "            All_labels , All_index, All_comp, All_var_ratio, \n",
    "            All_data, names, \n",
    "           All_lev1Ratio, All_lev2Ratio, All_lev3Ratio, kmeans, trans)\n",
    "i = 5\n",
    "FINAL = []\n",
    "if __name__ == \"__main__\": \n",
    "    pool = Pool(processes=2)\n",
    "    x = pool.map(main, (i,i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables = ['All_clust' ,'All_clust_space',\n",
    "             'All_labels' , 'All_index',\n",
    "             'All_comp', 'All_var_ratio',\n",
    "             'All_data', 'names',\n",
    "             'All_lev1Ratio', 'All_lev2Ratio',\n",
    "             'All_lev3Ratio', 'kmeans', 'DATA']\n",
    "# a, b in x\n",
    "dict1 ={}\n",
    "dict2 ={}\n",
    "dicts = [dict1, dict2]\n",
    "print(len(x))\n",
    "i=0\n",
    "for item in x:\n",
    "    dicti = dicts[i]\n",
    "    i=i+1\n",
    "    ii= 0\n",
    "    for value in item:\n",
    "        dicti[variables[ii]] = value\n",
    "        ii=ii+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1['All_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = [dict1, dict2]\n",
    "d = {}\n",
    "for k in dict1.iterkeys():\n",
    "    d[k] = tuple(d[k] for d in ds)\n",
    "\n",
    "All_lev1Ratio = np.vstack(d['All_lev1Ratio'])\n",
    "All_lev2Ratio = np.vstack(d['All_lev2Ratio'])\n",
    "All_lev3Ratio = np.vstack(d['All_lev3Ratio'])\n",
    "\n",
    "All_clust = np.hstack(d['All_clust'])\n",
    "All_clust_space = np.vstack(d['All_clust_space'])\n",
    "\n",
    "All_labels = np.hstack(d['All_labels'])\n",
    "All_index = np.hstack(d['All_index'])\n",
    "\n",
    "All_comp = np.vstack(d['All_comp'])\n",
    "# All_var_ratio = Var_ratio\n",
    "\n",
    "All_data = dict1['All_data'].reset_index().merge(dict2['All_data'], how=\"left\").set_index('index')\n",
    "\n",
    "All_DATA = np.vstack(d['DATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the variance from each K mean cluster and PCA, using PCA to find the most important factors over all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "All_Ratios = {'Lev1': All_lev1Ratio, 'Lev2': All_lev2Ratio, 'Lev3':All_lev3Ratio}\n",
    "Coefs = {}\n",
    "index = np.arange(11)\n",
    "for key, value in All_Ratios.iteritems(): \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit_transform(value)\n",
    "    i = np.identity(value.shape[1])  # identity matrix\n",
    "    Coefs[key] = pca.transform(i)\n",
    "    print(key)\n",
    "    print pd.DataFrame(pca.components_,columns=names[0:11],index = ['PC-1','PC-2'])\n",
    "    print('\\n')\n",
    "    \n",
    "    n_groups = 11\n",
    "\n",
    "    means_CP1 = np.absolute(pca.components_[0,:])\n",
    "    std_CP1 = np.matrix.std(np.matrix(np.absolute(pca.components_[0,:])))\n",
    "\n",
    "    means_CP2 = np.absolute(pca.components_[1,:])\n",
    "    std_CP2 = np.matrix.std(np.matrix(np.absolute(pca.components_[1,:])))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(45, 20))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = ax.bar(index, means_CP1, bar_width,\n",
    "                    alpha=opacity, color='b',\n",
    "                    yerr=std_CP1, error_kw=error_config,\n",
    "                    label='Component 1')\n",
    "\n",
    "    rects2 = ax.bar(index + bar_width, means_CP2, bar_width,\n",
    "                    alpha=opacity, color='r',\n",
    "                    yerr=std_CP2, error_kw=error_config,\n",
    "                    label='Component 2')\n",
    "    #plt.xlabel( fontsize=18)\n",
    "    ax.set_xlabel('Factors', fontsize = 50)\n",
    "    ax.set_ylabel('Coefficient Weight (Beta)', fontsize = 50)\n",
    "    ax.set_title('Which factors are the most meaning full for %s cluster'%key, fontsize = 50)\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    \n",
    "    ax.set_xticklabels(names[0:11], fontsize = 28, rotation = -15)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize = 38)\n",
    "\n",
    "    fig=plt.figure(figsize=(100, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging the original and new data sets on the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testData = pd.DataFrame(data = dudes)\n",
    "# print(dudes)\n",
    "# print(testData)\n",
    "# pd.merge(df1, df2, on = ['Name', 'Parent', 'Parent_Addr'], how = 'outer')\n",
    "\n",
    "total = All_data.reset_index().merge(testData, on = 'labels_true', how = 'left').set_index('index')\n",
    "print(list(All_data.columns.values))\n",
    "print(list(testData.columns.values))\n",
    "print(list(total.columns.values))\n",
    "# total['INDY'] = total.index\n",
    "total_g = total.groupby(['sub'],axis = 0).mean()\n",
    "total_g['labels_predR'] = total_g['labels_pred'].round()\n",
    "total_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "x['anthroweightcalc_y']#weight lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "T['hormone_scr_hse_meanR'] #testosterone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WC=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "WC['anthro_waist_cmR']# waist circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Skin=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "Skin['pds_skin2_yR']# skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_g['pds_skin2_yR'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Nice Plot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # print(All_data)\n",
    "# # df2 = pd.DataFrame(group.describe().rename(columns={'interview_age':labels_pred}).squeeze()\n",
    "# #                          for name, group in All_data.groupby('labels_pred'))\n",
    "\n",
    "# # print(df2)\n",
    "# subs=list(All_data.index.values)\n",
    "# len(subs)\n",
    "# All_data.interview_age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_groups = 3\n",
    "\n",
    "listOcool = ['pds_skin2_y_y', 'anthro_waist_cm_y', 'anthroweightcalc_y', 'hormone_scr_hse_mean_y']\n",
    "\n",
    "for item in listOcool:\n",
    "    means_CP1 = total_g['%s'%item].mean()\n",
    "    std_CP1 = total_g['%s'%item].std()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(45, 20))\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = ax.bar(index, means_CP1, bar_width,\n",
    "                    alpha=opacity, color='b',\n",
    "                    yerr=std_CP1, error_kw=error_config,\n",
    "                    label=item)\n",
    "\n",
    "    ax.set_xlabel('%s'%item, fontsize = 50)\n",
    "    ax.set_ylabel('Mean and Standard Deviation of %s'%item, fontsize = 50)\n",
    "    \n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(names[0:12], fontsize = 28)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize = 38)\n",
    "\n",
    "    fig=plt.figure(figsize=(100, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_g.to_csv('/Users/gracer/Google Drive/ABCD/important_txt/male_puberty_ana.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want this but colored the dots each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "varsy =[dudes, dict1, dict2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vars = {}\n",
    "keys = vars =['All_clust' ,'All_clust_space','All_labels' , \n",
    "              'All_index', 'All_comp', 'All_var_ratio','All_data', \n",
    "              'names',' All_lev1Ratio', 'All_lev2Ratio',' All_lev3Ratio', 'kmeans'] \n",
    "\n",
    "for i in range(0,len(keys)):\n",
    "    all_vars[keys[i]] = varsy[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = '/Users/gracer/Google Drive/ABCD/important_txt/male_data2.pickle'\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(all_vars, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print all_vars == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/Users/gracer/Google Drive/ABCD/important_txt/male_data2.pickle\", \"rb\") as input_file:\n",
    "#     x=pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All_clust (180,)<- length of the subjects over all runs\n",
    "* All_clust_space(180,)<-length of the subjects over all runs\n",
    "* All_means(180,)<-length of the subjects\n",
    "* All_labels(180,) <- length of subjects True Labels\n",
    "* All_index(180,)<-length of subjects Index of the original data\n",
    "* All_comp(18, 12)<- rows are the the PCA components, times the number of the levels, and the number of runs. cols are the factors\n",
    "* All_var_ratio(108, 2)<-rows are the number of runs, times the number of factors, times the number of levels\n",
    "* All_var(3, 6)<- rows are the number of runs, the columns are the PCA componenets times the number of levels\n",
    "* All_mean(9, 12)<- rows are the number of runs by the number of levels the columns are the number of factors\n",
    "* All_data(180, 14)<-rows are the number of subjects, the columns are number of factors plus the index, plus the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* components_ : array, shape (n_components, n_features)\n",
    "\n",
    "* explained_variance_ratio_ : array, shape (n_components,)\n",
    "\n",
    "*  singular_values_ : array, shape (n_components,)\n",
    "\n",
    "*  mean_ : array, shape (n_features,)\n",
    "\n",
    "*  n_components_ : int\n",
    "\n",
    "*  noise_variance_ : float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
