{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import Imputer\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "from multiprocess import pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data as csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_table('/Users/gracer/Google Drive/ABCD/important_txt/4Kmeans.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dudes = data[data['sex'] == 0]\n",
    "lady_dudes = data[data['sex'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a List of variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names=list(lady_dudes.columns.values)\n",
    "cols = lady_dudes.columns\n",
    "lady_dudes.columns = ['sub','pds_ht2_y',  'pds_skin2_y',  'pds_bdyhair_y','labels_true',\n",
    "            'pds_f4_2_y',  'pds_f5_y',  'pds_m4_y', 'pds_m5_y', 'interview_age', 'gender',\n",
    "             'anthroheightcalc',  'anthroweightcalc',  'anthro_waist_cm',\n",
    "             'hormone_scr_dhea_mean',  'hormone_scr_hse_mean',  'hormone_scr_ert_mean',  'sex']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine variables of interest into a single matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_true=lady_dudes['labels_true'].values\n",
    "f2=lady_dudes['pds_ht2_y'].values\n",
    "f3=lady_dudes['pds_skin2_y'].values\n",
    "f4=lady_dudes['pds_bdyhair_y'].values\n",
    "f5=lady_dudes['pds_f4_2_y'].values\n",
    "f6=lady_dudes['pds_f5_y'].values\n",
    "f7=lady_dudes['interview_age'].values\n",
    "f8=lady_dudes['anthroheightcalc'].values \n",
    "f9=lady_dudes['anthroweightcalc'].values\n",
    "f10=lady_dudes['anthro_waist_cm'].values\n",
    "f11=lady_dudes['hormone_scr_dhea_mean'].values\n",
    "f12=lady_dudes['hormone_scr_hse_mean'].values\n",
    "f13=lady_dudes['hormone_scr_ert_mean'].values\n",
    "X=np.matrix(zip(f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,f13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of the variable names included in this analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=['pds_ht2_y',\n",
    "'pds_skin2_y'\n",
    "'pds_bdyhair_y',\n",
    "'pds_f4_2_y',\n",
    "'pds_f5_y',\n",
    "'interview_age',\n",
    "'anthroheightcalc',\n",
    "'anthroweightcalc',\n",
    "'anthro_waist_cm',\n",
    "'hormone_scr_dhea_mean',\n",
    "'hormone_scr_hse_mean',\n",
    "'hormone_scr_ert_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(labels_true)\n",
    "y = np.bincount(labels_true.astype(int))\n",
    "ii = np.nonzero(y)[0]\n",
    "zip(ii,y[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue is a low number of people in groups 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible solution, randomly sample equal numbers\n",
    "Using the rule of thumb 2^m I need 8 people per cluster \n",
    "Possible combinations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x=math.factorial(70)\n",
    "y=math.factorial(70-20)\n",
    "fact=x/y\n",
    "print(fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target variable (or the one you are comparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_var=pd.DataFrame(lady_dudes['labels_true'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "This will not allow missing data, so have to impute nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "transformed_values = imputer.fit_transform(X)\n",
    "# count the number of NaN values in each column\n",
    "print(np.isnan(transformed_values).sum()) \n",
    "transformed_values_scale = scale(transformed_values)\n",
    "#the target variable is the last variable\n",
    "trans = np.hstack((transformed_values_scale,target_var.round(decimals=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to randomly sample the data and perform the kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bench_k_means(estimator, name, data):\n",
    "        t0 = time() #time\n",
    "        estimator.fit(data) #estimating the fit \n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=sample_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meaner(**kwargs):\n",
    "    for x in kwargs.items():\n",
    "        print(x)\n",
    "#     z=sum(*arg)/len(*arg)\n",
    "#     return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_reducer(DATA):\n",
    "    n_samples, n_features = DATA.shape\n",
    "    labels = np.round(targets)\n",
    "    n_digits = len(np.unique(targets))\n",
    "    sample_size=n_samples\n",
    "    \n",
    "    PCA_results = PCA(n_components=2)\n",
    "    reduced_data = PCA_results.fit_transform(DATA)\n",
    "    \n",
    "    # Dump components relations with features:\n",
    "    print pd.DataFrame(PCA_results.components_,index = ['PC-1','PC-2'])\n",
    "    plt.semilogy(PCA_results.explained_variance_ratio_, '--o')\n",
    "    return (PCA_results.components_, PCA_results.explained_variance_ratio_, \n",
    "            PCA_results.explained_variance_, PCA_results.mean_) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_selct(DATA,i):\n",
    "    print('start')\n",
    "    dictr = {}\n",
    "    #defining the levels, based on the final column add the values to the dictionary\n",
    "    levels = ['lev1','lev2','lev3']\n",
    "    i=1\n",
    "    for lev in levels:\n",
    "        if i < len(levels)+1:\n",
    "            dictr[lev] = DATA[np.where(DATA[:,-1] == i)]\n",
    "            i=i+1\n",
    "    \n",
    "    rand_dict={}\n",
    "    target_dict={}\n",
    "    ind_dict={}\n",
    "    for key, value in dictr.iteritems():\n",
    "        #shuffle the data's index\n",
    "        ind = np.random.permutation(value.shape[0])#random index\n",
    "        #get the first 20 subjects indexes\n",
    "        training_idx = ind[:50]#get 20 subjects indexes\n",
    "        #get the first 20 subjects\n",
    "        training = value[training_idx,:]#select 20 subjects from the value in the dictionary\n",
    "        #saving the true labels\n",
    "        labels_true = training[:,-1] #get the labels from the value in the dictiornary last column\n",
    "        target_dict[key] = labels_true #add targets to dictionary\n",
    "        rand_dict[key] = training #match the randomized data to the ind_dict by key \n",
    "        ind_dict[key] = training_idx #add the indexes to the dictionary\n",
    "    \n",
    "    '''\n",
    "    combine all the dictionaries we have created thus far. \n",
    "    data, index, and targets\n",
    "    '''\n",
    "    \n",
    "    #combine the randomized data by the actual level\n",
    "    data=np.vstack((rand_dict['lev1'],rand_dict['lev2'],rand_dict['lev3']))\n",
    "    \n",
    "    ###################################################\n",
    "    data=np.delete(data,12,1)# remove the label column\n",
    "    ###################################################\n",
    "    \n",
    "    \n",
    "    #combine the true labels into targets\n",
    "    targets=np.hstack((target_dict['lev1'],target_dict['lev2'],target_dict['lev3']))\n",
    "    #combine the indexes into one\n",
    "    indexes=np.hstack((ind_dict['lev1'],ind_dict['lev2'],ind_dict['lev3']))\n",
    "    \n",
    "    '''\n",
    "    defining the parameters for the k means and the PCA\n",
    "    '''\n",
    "    \n",
    "    n_samples, n_features = data.shape\n",
    "    labels = np.round(targets)\n",
    "    n_digits = len(np.unique(targets))\n",
    "    sample_size=n_samples\n",
    "\n",
    "    '''\n",
    "    defining the parameters for the k means\n",
    "    '''    \n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=300)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################\n",
    "    #            rebuild to combine data      #\n",
    "    ###########################################\n",
    "    comb_data = np.column_stack((data, kmeans.fit_predict(data)))#makes first column the index value\n",
    "    full_data = np.column_stack((indexes, comb_data))#makes first column the index value\n",
    "    full_data = np.column_stack((full_data,labels))\n",
    "    comb_dictr = {}\n",
    "    '''\n",
    "    seperating the data based on the fit predict value (found right above)\n",
    "    '''\n",
    "    i=0\n",
    "    for lev in levels:\n",
    "        if i < len(levels)+1:\n",
    "            comb_dictr[lev] = comb_data[np.where(comb_data[:,-1] == i)]\n",
    "            i=i+1\n",
    "    '''\n",
    "    defining the parameters for the PCA and running the PCA \n",
    "    '''\n",
    "    ###########################################\n",
    "    #            PCA                          #\n",
    "    ###########################################\n",
    "    PCA_dict={}\n",
    "    PCA_data={}\n",
    "    PCA_final = {}\n",
    "\n",
    "    for key, value in comb_dictr.iteritems():\n",
    "        PCA_dict[key] = PCA(n_components=2)\n",
    "        #make sure the last column which is the predicted label isn't included\n",
    "        PCA_data[key] = PCA_dict[key].fit_transform(value[:,0:12])\n",
    "        #make sure the last column which is the predicted label isn't included\n",
    "        ID = np.identity(value[:,0:12].shape[1])  # identity matrix\n",
    "        coef = PCA_dict[key].transform(ID)\n",
    "        PCA_final[key] = coef\n",
    "        \n",
    "        plt.semilogy(PCA_dict[key].explained_variance_ratio_, '--o', label = '%s'%key)\n",
    "#     plt.legend()        \n",
    "\n",
    "    reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "    kmeans.fit(reduced_data)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "              'Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    ####THINGS TO SAVE#############################################################################        \n",
    "    kmeans.fit(data)\n",
    "    y_kmeans = kmeans.predict(data)\n",
    "    clusters = kmeans.fit_predict(data)\n",
    "    cluster_space = kmeans.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    centroids = kmeans.cluster_centers_   \n",
    "    \n",
    "    Components = np.vstack((PCA_dict['lev1'].components_,\n",
    "                            PCA_dict['lev2'].components_,PCA_dict['lev3'].components_))\n",
    "    \n",
    "    Var_ratio = PCA_final\n",
    "    \n",
    "    lev1_VarRatio = np.vstack((PCA_final['lev1']))\n",
    "    lev2_VarRatio = np.vstack((PCA_final['lev2']))\n",
    "    lev3_VarRatio = np.vstack((PCA_final['lev3']))\n",
    "    \n",
    "    ind_dict = np.hstack((ind_dict['lev1'],\n",
    "                            ind_dict['lev2'],ind_dict['lev3']))\n",
    "    \n",
    "    ########################################################################################################\n",
    "    return (clusters, cluster_space, centroids, \n",
    "            targets, ind_dict,\n",
    "           Components, Var_ratio, \n",
    "           full_data,\n",
    "           lev1_VarRatio,\n",
    "           lev2_VarRatio,\n",
    "           lev3_VarRatio,\n",
    "           kmeans,\n",
    "           DATA)\n",
    "\n",
    "# 0clusters\n",
    "# 1cluster_space, \n",
    "# 2centroids, \n",
    "# 3targets, \n",
    "# 4ind_dict,\n",
    "# 5Components,\n",
    "# 6Var_ratio, \n",
    "# 7full_data,\n",
    "# 8lev1_VarRatio,\n",
    "# 9lev2_VarRatio,\n",
    "# 10lev3_VarRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(i):\n",
    "    names = ['pds_ht2_y',\n",
    "'pds_skin2_y',\n",
    "'pds_bdyhair_y',\n",
    "'pds_f4_2_y',\n",
    "'pds_f5_y',\n",
    "'interview_age',\n",
    "'anthroheightcalc', \n",
    "'anthroweightcalc',\n",
    "'anthro_waist_cm',\n",
    "'hormone_scr_dhea_mean',\n",
    "'hormone_scr_hse_mean',\n",
    "'hormone_scr_ert_mean', 'labels_pred','labels_true']\n",
    "    \n",
    "    Clusters = []\n",
    "    Cluster_space = []\n",
    "    Centroids = []\n",
    "    \n",
    "    Comp = []\n",
    "    Var_ratio = []\n",
    "    \n",
    "    Targets = []\n",
    "    Indexes = []\n",
    "    \n",
    "    Full_data = []\n",
    "    \n",
    "    Lev1_VarRatio = []\n",
    "    Lev2_VarRatio = []\n",
    "    Lev3_VarRatio = []\n",
    "    \n",
    "    kmeans = []\n",
    "    for x in range(0,i):\n",
    "        Q = random_selct(trans,i)\n",
    "        Clusters.append(Q[0])\n",
    "        Cluster_space.append(Q[1])\n",
    "        Centroids.append(Q[2])\n",
    "        \n",
    "        Targets.append(Q[3])\n",
    "        Indexes.append(Q[4])\n",
    "        \n",
    "        Comp.append(Q[5])\n",
    "        Var_ratio.append(Q[6])\n",
    "        \n",
    "        Full_data.append(Q[7])\n",
    "        \n",
    "        Lev1_VarRatio.append(Q[8].transpose())\n",
    "        Lev2_VarRatio.append(Q[9].transpose())\n",
    "        Lev3_VarRatio.append(Q[10].transpose())\n",
    "        \n",
    "        kmeans.append(Q[11])\n",
    "    All_lev1Ratio = np.vstack(Lev1_VarRatio)\n",
    "    All_lev2Ratio = np.vstack(Lev2_VarRatio)\n",
    "    All_lev3Ratio = np.vstack(Lev3_VarRatio)\n",
    "    \n",
    "    All_clust = np.hstack(Clusters)\n",
    "    All_clust_space = np.vstack(Cluster_space)\n",
    "    \n",
    "    All_labels = np.hstack(Targets)\n",
    "    All_index = np.hstack(Indexes)\n",
    "    \n",
    "    All_comp = np.vstack(Comp)\n",
    "    All_var_ratio = Var_ratio\n",
    "    All_data = np.vstack(Full_data)\n",
    "    \n",
    "    print(All_data.shape)\n",
    "    All_data=pd.DataFrame(data=All_data[1:,1:],index=All_data[1:,0], columns= names)\n",
    "    return (All_clust ,All_clust_space,  \n",
    "            All_labels , All_index, All_comp, All_var_ratio, \n",
    "            All_data, names, \n",
    "           All_lev1Ratio, All_lev2Ratio, All_lev3Ratio, kmeans, trans)\n",
    "i = 5000\n",
    "FINAL = []\n",
    "if __name__ == \"__main__\": \n",
    "    pool = Pool(processes=2)\n",
    "    x = pool.map(main, (i,i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables = ['All_clust' ,'All_clust_space',\n",
    "             'All_labels' , 'All_index',\n",
    "             'All_comp', 'All_var_ratio',\n",
    "             'All_data', 'names',\n",
    "             'All_lev1Ratio', 'All_lev2Ratio',\n",
    "             'All_lev3Ratio', 'kmeans', 'DATA']\n",
    "# a, b in x\n",
    "dict1 ={}\n",
    "dict2 ={}\n",
    "dicts = [dict1, dict2]\n",
    "print(len(x))\n",
    "i=0\n",
    "for item in x:\n",
    "    dicti = dicts[i]\n",
    "    i=i+1\n",
    "    ii= 0\n",
    "    for value in item:\n",
    "        dicti[variables[ii]] = value\n",
    "        ii=ii+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = [dict1, dict2]\n",
    "d = {}\n",
    "for k in dict1.iterkeys():\n",
    "    d[k] = tuple(d[k] for d in ds)\n",
    "\n",
    "All_lev1Ratio = np.vstack(d['All_lev1Ratio'])\n",
    "All_lev2Ratio = np.vstack(d['All_lev2Ratio'])\n",
    "All_lev3Ratio = np.vstack(d['All_lev3Ratio'])\n",
    "\n",
    "All_clust = np.hstack(d['All_clust'])\n",
    "All_clust_space = np.vstack(d['All_clust_space'])\n",
    "\n",
    "All_labels = np.hstack(d['All_labels'])\n",
    "All_index = np.hstack(d['All_index'])\n",
    "\n",
    "All_comp = np.vstack(d['All_comp'])\n",
    "# All_var_ratio = Var_ratio\n",
    "\n",
    "All_data = dict1['All_data'].reset_index().merge(dict2['All_data'], how=\"left\").set_index('index')\n",
    "\n",
    "All_DATA = np.vstack(d['DATA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the variance from each K mean cluster and PCA, using PCA to find the most important factors over all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "All_Ratios = {'Lev1': All_lev1Ratio, 'Lev2': All_lev2Ratio, 'Lev3':All_lev3Ratio}\n",
    "Coefs = {}\n",
    "index = np.arange(12)\n",
    "for key, value in All_Ratios.iteritems(): \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit_transform(value)\n",
    "    i = np.identity(value.shape[1])  # identity matrix\n",
    "    Coefs[key] = pca.transform(i)\n",
    "    print(key)\n",
    "    print pd.DataFrame(pca.components_,columns=names[0:12],index = ['PC-1','PC-2'])\n",
    "    print('\\n')\n",
    "    \n",
    "    n_groups = 12\n",
    "\n",
    "    means_CP1 = np.absolute(pca.components_[0,:])\n",
    "    std_CP1 = np.matrix.std(np.matrix(np.absolute(pca.components_[0,:])))\n",
    "\n",
    "    means_CP2 = np.absolute(pca.components_[1,:])\n",
    "    std_CP2 = np.matrix.std(np.matrix(np.absolute(pca.components_[1,:])))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(45, 20))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = ax.bar(index, means_CP1, bar_width,\n",
    "                    alpha=opacity, color='b',\n",
    "                    yerr=std_CP1, error_kw=error_config,\n",
    "                    label='Component 1')\n",
    "\n",
    "    rects2 = ax.bar(index + bar_width, means_CP2, bar_width,\n",
    "                    alpha=opacity, color='r',\n",
    "                    yerr=std_CP2, error_kw=error_config,\n",
    "                    label='Component 2')\n",
    "    #plt.xlabel( fontsize=18)\n",
    "    ax.set_xlabel('Factors', fontsize = 50)\n",
    "    ax.set_ylabel('Coefficient Weight (Beta)', fontsize = 50)\n",
    "    ax.set_title('Which factors are the most meaning full for %s cluster'%key, fontsize = 50)\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    \n",
    "    ax.set_xticklabels(names[0:12], fontsize = 28, rotation = -15)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize = 38)\n",
    "\n",
    "    fig=plt.figure(figsize=(100, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging the original and new data sets on the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names2=['pds_ht2_yR',\n",
    "'pds_skin2_yR',\n",
    "'pds_bdyhair_yR',\n",
    "'pds_f4_2_yR',\n",
    "'pds_f5_yR',\n",
    "'interview_ageR',\n",
    "'anthroheightcalcR', \n",
    "'anthroweightcalcR',\n",
    "'anthro_waist_cmR',\n",
    "'hormone_scr_dhea_meanR',\n",
    "'hormone_scr_hse_meanR',\n",
    "'hormone_scr_ert_meanR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData = pd.DataFrame(data = lady_dudes)\n",
    "# print(dudes)\n",
    "# print(testData)\n",
    "# pd.merge(df1, df2, on = ['Name', 'Parent', 'Parent_Addr'], how = 'outer')\n",
    "\n",
    "total = All_data.reset_index().merge(testData, on = 'labels_true', how = 'left').set_index('index')\n",
    "print(list(All_data.columns.values))\n",
    "print(list(testData.columns.values))\n",
    "print(list(total.columns.values))\n",
    "# total['INDY'] = total.index\n",
    "total_g = total.groupby(['sub'],axis = 0).mean()\n",
    "total_g['labels_predR'] = total_g['labels_pred'].round()\n",
    "total_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "x['anthroweightcalcR']#weight lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "T['hormone_scr_hse_meanR'] #testosterone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "WC['anthro_waist_cmR']# waist circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skin=total_g.groupby(['labels_predR'], axis =0).describe()\n",
    "Skin['pds_skin2_yR']# skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_g['pds_skin2_yR'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Nice Plot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # print(All_data)\n",
    "# # df2 = pd.DataFrame(group.describe().rename(columns={'interview_age':labels_pred}).squeeze()\n",
    "# #                          for name, group in All_data.groupby('labels_pred'))\n",
    "\n",
    "# # print(df2)\n",
    "# subs=list(All_data.index.values)\n",
    "# len(subs)\n",
    "# All_data.interview_age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_groups = 3\n",
    "\n",
    "listOcool = ['pds_skin2_y_y', 'anthro_waist_cm_y', 'anthroweightcalc_y', 'hormone_scr_hse_mean_y']\n",
    "\n",
    "for item in listOcool:\n",
    "    means_CP1 = total_g['%s'%item].mean()\n",
    "    std_CP1 = total_g['%s'%item].std()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(45, 20))\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = ax.bar(index, means_CP1, bar_width,\n",
    "                    alpha=opacity, color='b',\n",
    "                    yerr=std_CP1, error_kw=error_config,\n",
    "                    label=item)\n",
    "\n",
    "    ax.set_xlabel('%s'%item, fontsize = 50)\n",
    "    ax.set_ylabel('Mean and Standard Deviation of %s'%item, fontsize = 50)\n",
    "    \n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(names[0:12], fontsize = 28)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize = 38)\n",
    "\n",
    "    fig=plt.figure(figsize=(100, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want this but colored the dots each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/Users/gracer/Google Drive/ABCD/important_txt/data.pickle\", \"rb\") as input_file:\n",
    "    x=pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['All_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All_clust (180,)<- length of the subjects over all runs\n",
    "* All_clust_space(180,)<-length of the subjects over all runs\n",
    "* All_means(180,)<-length of the subjects\n",
    "* All_labels(180,) <- length of subjects True Labels\n",
    "* All_index(180,)<-length of subjects Index of the original data\n",
    "* All_comp(18, 12)<- rows are the the PCA components, times the number of the levels, and the number of runs. cols are the factors\n",
    "* All_var_ratio(108, 2)<-rows are the number of runs, times the number of factors, times the number of levels\n",
    "* All_var(3, 6)<- rows are the number of runs, the columns are the PCA componenets times the number of levels\n",
    "* All_mean(9, 12)<- rows are the number of runs by the number of levels the columns are the number of factors\n",
    "* All_data(180, 14)<-rows are the number of subjects, the columns are number of factors plus the index, plus the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* components_ : array, shape (n_components, n_features)\n",
    "\n",
    "* explained_variance_ratio_ : array, shape (n_components,)\n",
    "\n",
    "*  singular_values_ : array, shape (n_components,)\n",
    "\n",
    "*  mean_ : array, shape (n_features,)\n",
    "\n",
    "*  n_components_ : int\n",
    "\n",
    "*  noise_variance_ : float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
