{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import Imputer\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "matplotlib.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data as csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_table('/Users/gracer/Google Drive/ABCD/important_txt/4Kmeans.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dudes = data[data['sex'] >= 0]\n",
    "lady_dudes = data[data['sex'] >= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a List of variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names=list(lady_dudes.columns.values)\n",
    "cols = lady_dudes.columns\n",
    "print(cols)\n",
    "cols[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine variables of interest into a single matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true=lady_dudes['PDS'].values\n",
    "f2=lady_dudes['pds_ht2_y'].values\n",
    "f3=lady_dudes['pds_skin2_y'].values\n",
    "f4=lady_dudes['pds_bdyhair_y'].values\n",
    "f5=lady_dudes['pds_f4_2_y'].values\n",
    "f6=lady_dudes['pds_f5_y'].values\n",
    "f7=lady_dudes['interview_age'].values\n",
    "f8=lady_dudes['anthroheightcalc'].values \n",
    "f9=lady_dudes['anthroweightcalc'].values\n",
    "f10=lady_dudes['anthro_waist_cm'].values\n",
    "f11=lady_dudes['hormone_scr_dhea_mean'].values\n",
    "f12=lady_dudes['hormone_scr_hse_mean'].values\n",
    "f13=lady_dudes['hormone_scr_ert_mean'].values\n",
    "X=np.matrix(zip(f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,f13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of the variable names included in this analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['pds_ht2_y',\n",
    "'pds_skin2_y'\n",
    "'pds_bdyhair_y',\n",
    "'pds_f4_2_y',\n",
    "'pds_f5_y',\n",
    "'interview_age',\n",
    "'anthroheightcalc',\n",
    "'anthroweightcalc',\n",
    "'anthro_waist_cm',\n",
    "'hormone_scr_dhea_mean',\n",
    "'hormone_scr_hse_mean',\n",
    "'hormone_scr_ert_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(labels_true)\n",
    "y = np.bincount(labels_true.astype(int))\n",
    "ii = np.nonzero(y)[0]\n",
    "zip(ii,y[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue is a low number of people in groups 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible solution, randomly sample equal numbers\n",
    "Using the rule of thumb 2^m I need 8 people per cluster \n",
    "Possible combinations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x=math.factorial(70)\n",
    "y=math.factorial(70-20)\n",
    "fact=x/y\n",
    "print(fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target variable (or the one you are comparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var=pd.DataFrame(lady_dudes['PDS'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "This will not allow missing data, so have to impute nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "transformed_values = imputer.fit_transform(X)\n",
    "# count the number of NaN values in each column\n",
    "print(np.isnan(transformed_values).sum()) \n",
    "transformed_values_scale = scale(transformed_values)\n",
    "#the target variable is the last variable\n",
    "trans = np.hstack((transformed_values_scale,target_var.round(decimals=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to randomly sample the data and perform the kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_k_means(estimator, name, data):\n",
    "        t0 = time() #time\n",
    "        estimator.fit(data) #estimating the fit \n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=sample_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meaner(**kwargs):\n",
    "    for x in kwargs.items():\n",
    "        print(x)\n",
    "#     z=sum(*arg)/len(*arg)\n",
    "#     return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_reducer(DATA):\n",
    "    n_samples, n_features = DATA.shape\n",
    "    labels = np.round(targets)\n",
    "    n_digits = len(np.unique(targets))\n",
    "    sample_size=n_samples\n",
    "    \n",
    "    PCA_results = PCA(n_components=2)\n",
    "    reduced_data = PCA_results.fit_transform(DATA)\n",
    "    \n",
    "    # Dump components relations with features:\n",
    "    print pd.DataFrame(PCA_results.components_,index = ['PC-1','PC-2'])\n",
    "    plt.semilogy(PCA_results.explained_variance_ratio_, '--o')\n",
    "    return (PCA_results.components_, PCA_results.explained_variance_ratio_, \n",
    "            PCA_results.explained_variance_, PCA_results.mean_) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_selct(DATA,i):\n",
    "    print('start')\n",
    "    dictr = {}\n",
    "    #defining the levels, based on the final column add the values to the dictionary\n",
    "    levels = ['lev1','lev2','lev3']\n",
    "    i=1\n",
    "    for lev in levels:\n",
    "        if i < len(levels)+1:\n",
    "            dictr[lev] = DATA[np.where(DATA[:,-1] == i)]\n",
    "            i=i+1\n",
    "    \n",
    "    rand_dict={}\n",
    "    target_dict={}\n",
    "    ind_dict={}\n",
    "    for key, value in dictr.iteritems():\n",
    "        #shuffle the data's index\n",
    "        ind = np.random.permutation(value.shape[0])#random index\n",
    "        #get the first 20 subjects indexes\n",
    "        training_idx = ind[:20]#get 20 subjects indexes\n",
    "        #get the first 20 subjects\n",
    "        training = value[training_idx,:]#select 20 subjects from the value in the dictionary\n",
    "        #saving the true labels\n",
    "        labels_true = training[:,-1] #get the labels from the value in the dictiornary last column\n",
    "        target_dict[key] = labels_true #add targets to dictionary\n",
    "        rand_dict[key] = training #match the randomized data to the ind_dict by key \n",
    "        ind_dict[key] = training_idx #add the indexes to the dictionary\n",
    "    \n",
    "    '''\n",
    "    combine all the dictionaries we have created thus far. \n",
    "    data, index, and targets\n",
    "    '''\n",
    "    \n",
    "    #combine the randomized data by the actual level\n",
    "    data=np.vstack((rand_dict['lev1'],rand_dict['lev2'],rand_dict['lev3']))\n",
    "    \n",
    "    ###################################################\n",
    "    data=np.delete(data,12,1)# remove the label column\n",
    "    ###################################################\n",
    "    \n",
    "    \n",
    "    #combine the true labels into targets\n",
    "    targets=np.hstack((target_dict['lev1'],target_dict['lev2'],target_dict['lev3']))\n",
    "    #combine the indexes into one\n",
    "    indexes=np.hstack((ind_dict['lev1'],ind_dict['lev2'],ind_dict['lev3']))\n",
    "    \n",
    "    '''\n",
    "    defining the parameters for the k means and the PCA\n",
    "    '''\n",
    "    \n",
    "    n_samples, n_features = data.shape\n",
    "    labels = np.round(targets)\n",
    "    n_digits = len(np.unique(targets))\n",
    "    sample_size=n_samples\n",
    "\n",
    "    '''\n",
    "    defining the parameters for the k means\n",
    "    '''    \n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=300)\n",
    "    \n",
    "    ###########################################\n",
    "    #            rebuild to combine data      #\n",
    "    ###########################################\n",
    "    comb_data = np.column_stack((data, kmeans.fit_predict(data)))#makes first column the index value\n",
    "    full_data = np.column_stack((indexes, comb_data))#makes first column the index value\n",
    "    comb_dictr = {}\n",
    "    '''\n",
    "    seperating the data based on the fit predict value (found right above)\n",
    "    '''\n",
    "    i=0\n",
    "    for lev in levels:\n",
    "        if i < len(levels)+1:\n",
    "            comb_dictr[lev] = comb_data[np.where(comb_data[:,-1] == i)]\n",
    "            i=i+1\n",
    "    '''\n",
    "    defining the parameters for the PCA and running the PCA \n",
    "    '''\n",
    "    ###########################################\n",
    "    #            PCA                          #\n",
    "    ###########################################\n",
    "    PCA_dict={}\n",
    "    PCA_data={}\n",
    "    PCA_final = {}\n",
    "\n",
    "    for key, value in comb_dictr.iteritems():\n",
    "        PCA_dict[key] = PCA(n_components=2)\n",
    "        #make sure the last column which is the predicted label isn't included\n",
    "#         print(value[:,0:12])\n",
    "        PCA_data[key] = PCA_dict[key].fit_transform(value[:,0:12])\n",
    "        #make sure the last column which is the predicted label isn't included\n",
    "        ID = np.identity(value[:,0:12].shape[1])  # identity matrix\n",
    "        coef = PCA_dict[key].transform(ID)\n",
    "        PCA_final[key] = coef\n",
    "        \n",
    "        plt.semilogy(PCA_dict[key].explained_variance_ratio_, '--o', label = '%s'%key)\n",
    "    plt.legend()        \n",
    "        \n",
    "    \n",
    "\n",
    "    ####THINGS TO SAVE#############################################################################        \n",
    "    kmeans.fit(data)\n",
    "    y_kmeans = kmeans.predict(data)\n",
    "    clusters = kmeans.fit_predict(data)\n",
    "    cluster_space = kmeans.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    centroids = kmeans.cluster_centers_   \n",
    "    \n",
    "    Components = np.vstack((PCA_dict['lev1'].components_,\n",
    "                            PCA_dict['lev2'].components_,PCA_dict['lev3'].components_))\n",
    "    \n",
    "#     Var_ratio = np.vstack((PCA_final['lev1'],\n",
    "#                              PCA_final['lev2'],PCA_final['lev3']))\n",
    "    Var_ratio = PCA_final\n",
    "    \n",
    "    lev1_VarRatio = np.vstack((PCA_final['lev1']))\n",
    "    print(lev1_VarRatio)\n",
    "    lev2_VarRatio = np.vstack((PCA_final['lev2']))\n",
    "    lev3_VarRatio = np.vstack((PCA_final['lev3']))\n",
    "    \n",
    "#     Var = np.vstack((PCA_dict['lev1'].explained_variance_,\n",
    "#                            PCA_dict['lev2'].explained_variance_,PCA_dict['lev3'].explained_variance_))            \n",
    "    Mean = np.vstack((PCA_dict['lev1'].mean_,\n",
    "                           PCA_dict['lev2'].mean_,PCA_dict['lev3'].mean_))                          \n",
    "    ind_dict = np.hstack((ind_dict['lev1'],\n",
    "                            ind_dict['lev2'],ind_dict['lev3']))\n",
    "    \n",
    "    ########################################################################################################\n",
    "    return (clusters, cluster_space, centroids, \n",
    "            targets, ind_dict,\n",
    "           Components, Var_ratio, \n",
    "           full_data,\n",
    "           lev1_VarRatio,\n",
    "           lev2_VarRatio,\n",
    "           lev3_VarRatio)\n",
    "\n",
    "# 0clusters\n",
    "# 1cluster_space, \n",
    "# 2centroids, \n",
    "# 3targets, \n",
    "# 4ind_dict,\n",
    "# 5Components,\n",
    "# 6Var_ratio, \n",
    "# 7full_data,\n",
    "# 8lev1_VarRatio,\n",
    "# 9lev2_VarRatio,\n",
    "# 10lev3_VarRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(i):\n",
    "    names = ['pds_ht2_y',\n",
    "'pds_skin2_y',\n",
    "'pds_bdyhair_y',\n",
    "'pds_f4_2_y',\n",
    "'pds_f5_y',\n",
    "'interview_age',\n",
    "'anthroheightcalc', \n",
    "'anthroweightcalc',\n",
    "'anthro_waist_cm',\n",
    "'hormone_scr_dhea_mean',\n",
    "'hormone_scr_hse_mean',\n",
    "'hormone_scr_ert_mean', 'labels_pred']\n",
    "    \n",
    "#     Y_means = []\n",
    "    Clusters = []\n",
    "    Cluster_space = []\n",
    "    Centroids = []\n",
    "    \n",
    "    Comp = []\n",
    "    Var_ratio = []\n",
    "#     Var = []\n",
    "#     Mean = []\n",
    "    Targets = []\n",
    "    Indexes = []\n",
    "    Full_data = []\n",
    "    Lev1_VarRatio = []\n",
    "    Lev2_VarRatio = []\n",
    "    Lev3_VarRatio = []\n",
    "    \n",
    "    for x in range(0,i):\n",
    "        Q = random_selct(trans,i)\n",
    "#         Y_means.append(Q[0])\n",
    "        Clusters.append(Q[0])\n",
    "        Cluster_space.append(Q[1])\n",
    "        Centroids.append(Q[2])\n",
    "        Targets.append(Q[3])\n",
    "        Indexes.append(Q[4])\n",
    "#         Var.append(Q[8])\n",
    "        Comp.append(Q[5])\n",
    "        Var_ratio.append(Q[6])\n",
    "        Full_data.append(Q[7])\n",
    "        Lev1_VarRatio.append(Q[8].transpose())\n",
    "        print(Lev1_VarRatio)\n",
    "        Lev2_VarRatio.append(Q[9].transpose())\n",
    "        Lev3_VarRatio.append(Q[10].transpose())\n",
    "    \n",
    "    All_lev1Ratio = np.vstack(Lev1_VarRatio)\n",
    "    print(All_lev1Ratio)\n",
    "    All_lev2Ratio = np.vstack(Lev2_VarRatio)\n",
    "    All_lev3Ratio = np.vstack(Lev3_VarRatio)\n",
    "    \n",
    "    All_clust = np.hstack(Clusters)\n",
    "    All_clust_space = np.vstack(Cluster_space)\n",
    "#     All_means = np.hstack(Y_means)\n",
    "    All_labels = np.hstack(Targets)\n",
    "    All_index = np.hstack(Indexes)\n",
    "    All_comp = np.vstack(Comp)\n",
    "#    All_var_ratio = np.vstack(Var_ratio)\n",
    "    All_var_ratio = Var_ratio\n",
    "#     All_var = np.hstack(Var)\n",
    "    All_data = np.vstack(Full_data)\n",
    "    \n",
    "#     All_data=pd.DataFrame(data=All_data[1:,1:],index=All_data[1:,0], columns= names)\n",
    "#    New_names = ['mean','true_labels']\n",
    "#    New_data=np.vstack((All_index, All_labels))\n",
    "#     New_data=New_data.transpose()\n",
    "#     New_data=pd.DataFrame(data=New_data[1:,1:],index=New_data[1:,0],columns = New_names)\n",
    "    \n",
    "    return (All_clust ,All_clust_space,  \n",
    "            All_labels , All_index, All_comp, All_var_ratio, \n",
    "            All_data, names, \n",
    "           All_lev1Ratio, All_lev2Ratio, All_lev3Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "All_clust ,All_clust_space,All_labels , All_index, All_comp, All_var_ratio,All_data, names, All_lev1Ratio, All_lev2Ratio, All_lev3Ratio = main(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed_values\n",
    "# comb_data = np.column_stack((indexes, transformed_values))#makes first column the index value\n",
    "# comb_data = np.column_stack((comb_data, kmeans.fit_predict(data)))\n",
    "# pd.DataFrame(data=transformed[1:,1:],index=All_data[1:,0], columns= names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_lev1Ratio.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the variance from each K mean cluster and PCA, using PCA to find the most important factors over all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "All_Ratios = {'Lev1': All_lev1Ratio, 'Lev2': All_lev2Ratio, 'Lev3':All_lev3Ratio}\n",
    "Coefs = {}\n",
    "index = np.arange(12)\n",
    "for key, value in All_Ratios.iteritems(): \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit_transform(value)\n",
    "    i = np.identity(value.shape[1])  # identity matrix\n",
    "    Coefs[key] = pca.transform(i)\n",
    "    print(key)\n",
    "    print pd.DataFrame(pca.components_,columns=names[0:12],index = ['PC-1','PC-2'])\n",
    "    print('\\n')\n",
    "    \n",
    "    n_groups = 12\n",
    "\n",
    "    means_CP1 = np.absolute(pca.components_[0,:])\n",
    "    std_CP1 = np.matrix.std(np.matrix(np.absolute(pca.components_[0,:])))\n",
    "\n",
    "    means_CP2 = np.absolute(pca.components_[1,:])\n",
    "    std_CP2 = np.matrix.std(np.matrix(np.absolute(pca.components_[1,:])))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(45, 20))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = ax.bar(index, means_CP1, bar_width,\n",
    "                    alpha=opacity, color='b',\n",
    "                    yerr=std_CP1, error_kw=error_config,\n",
    "                    label='Component 1')\n",
    "\n",
    "    rects2 = ax.bar(index + bar_width, means_CP2, bar_width,\n",
    "                    alpha=opacity, color='r',\n",
    "                    yerr=std_CP2, error_kw=error_config,\n",
    "                    label='Component 2')\n",
    "    #plt.xlabel( fontsize=18)\n",
    "    ax.set_xlabel('Factors', fontsize = 50)\n",
    "    ax.set_ylabel('Coefficient Weight (Beta)', fontsize = 50)\n",
    "    ax.set_title('Which factors are the most meaning full for %s cluster'%key, fontsize = 50)\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(names[0:12], fontsize = 28, rotation = 25)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize = 38)\n",
    "\n",
    "    fig=plt.figure(figsize=(100, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Nice Plot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # print(All_data)\n",
    "# # df2 = pd.DataFrame(group.describe().rename(columns={'interview_age':labels_pred}).squeeze()\n",
    "# #                          for name, group in All_data.groupby('labels_pred'))\n",
    "\n",
    "# # print(df2)\n",
    "# subs=list(All_data.index.values)\n",
    "# len(subs)\n",
    "# All_data.interview_age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "# from collections import namedtuple\n",
    "# matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "\n",
    "# n_groups = 12\n",
    "\n",
    "# means_CP1 = np.absolute(pca.components_[0,:])\n",
    "# std_CP1 = np.matrix.std(np.matrix(np.absolute(pca.components_[0,:])))\n",
    "\n",
    "# means_CP2 = np.absolute(pca.components_[1,:])\n",
    "# std_CP2 = np.matrix.std(np.matrix(np.absolute(pca.components_[1,:])))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(45, 20))\n",
    "\n",
    "\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.4\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# rects1 = ax.bar(index, means_CP1, bar_width,\n",
    "#                 alpha=opacity, color='b',\n",
    "#                 yerr=std_CP1, error_kw=error_config,\n",
    "#                 label='Component 1')\n",
    "\n",
    "# rects2 = ax.bar(index + bar_width, means_CP2, bar_width,\n",
    "#                 alpha=opacity, color='r',\n",
    "#                 yerr=std_CP2, error_kw=error_config,\n",
    "#                 label='Component 2')\n",
    "# #plt.xlabel( fontsize=18)\n",
    "# ax.set_xlabel('Factors', fontsize = 50)\n",
    "# ax.set_ylabel('Coefficient Weight (Beta)', fontsize = 50)\n",
    "# ax.set_title('Which factors are the most meaning full for Lev 1 cluster', fontsize = 50)\n",
    "# ax.set_xticks(index + bar_width / 2)\n",
    "# ax.set_xticklabels(names[0:12], fontsize = 28, rotation = 25)\n",
    "# ax.legend()\n",
    "# ax.tick_params(labelsize = 38)\n",
    "\n",
    "# fig=plt.figure(figsize=(100, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All_clust (180,)<- length of the subjects over all runs\n",
    "* All_clust_space(180,)<-length of the subjects over all runs\n",
    "* All_means(180,)<-length of the subjects\n",
    "* All_labels(180,) <- length of subjects True Labels\n",
    "* All_index(180,)<-length of subjects Index of the original data\n",
    "* All_comp(18, 12)<- rows are the the PCA components, times the number of the levels, and the number of runs. cols are the factors\n",
    "* All_var_ratio(108, 2)<-rows are the number of runs, times the number of factors, times the number of levels\n",
    "* All_var(3, 6)<- rows are the number of runs, the columns are the PCA componenets times the number of levels\n",
    "* All_mean(9, 12)<- rows are the number of runs by the number of levels the columns are the number of factors\n",
    "* All_data(180, 14)<-rows are the number of subjects, the columns are number of factors plus the index, plus the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* components_ : array, shape (n_components, n_features)\n",
    "\n",
    "* explained_variance_ratio_ : array, shape (n_components,)\n",
    "\n",
    "*  singular_values_ : array, shape (n_components,)\n",
    "\n",
    "*  mean_ : array, shape (n_features,)\n",
    "\n",
    "*  n_components_ : int\n",
    "\n",
    "*  noise_variance_ : float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
